{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249d9f2-a705-4f1e-9bac-4a86e6468679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "# Set field size limit to the maximum possible\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import ast\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "from read_data import *\n",
    "from gpcam import GPOptimizer\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from functools import partial\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import time\n",
    "from gpAMRutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd4591-be09-407d-b1e8-907091c6c08d",
   "metadata": {},
   "source": [
    "## Tuning options to consider\n",
    "- Filtering data by block or as a function of the global data?\n",
    "- What fraction of candidates should be used, > mean SD? top 10%, top n candidates?\n",
    "- How will data noise be estimated and set?\n",
    "- RAM: submitting ask several times means that $\\kappa \\in \\mathbb{R}^{12000 \\times 12000}$ is possibly stored many times at once. At the same time the interpolator in the kernel stores temporarily 2(3) matrices of shape (len(x_data)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85100655-9117-46b9-b44d-ca62b9d2356b",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9b443-3b02-434a-9118-62dfe57b893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings:\n",
    "number_of_workers = 16\n",
    "number_of_threads = 32\n",
    "\n",
    "tol_ratio = 0.00001 #1e-2 #remove data < tol_ratio * max(data)\n",
    "refinement_res = 2 #refinement level\n",
    "plotting = True #plot posterior mean and var\n",
    "plot_resx = 100\n",
    "plot_resy = 100\n",
    "\n",
    "#filename = \"plot.nx256.2d.AMRLevel.hdf5\"\n",
    "filename = \"plot.nx64.2d.AMRLevel.hdf5\"\n",
    "index = \"density\"\n",
    "percentile = 90 ##of all candidates considered, return the top percentile\n",
    "\n",
    "chombo_path = \"./ChomboOut/\"\n",
    "gpcam_path = \"./gpCAMOut/\"\n",
    "\n",
    "#data_folder = '../data_gpamrII/'\n",
    "#file_list = sorted([data_folder+f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))])\n",
    "\n",
    "init_hyperparameters = np.array([1., 50., 0.4])\n",
    "hyperparameter_bounds = np.array([[0.00001, 1.],\n",
    "                                  [0.1, 100.],\n",
    "                                  [0.1, 0.5],])\n",
    "#print(len(file_list))\n",
    "if os.path.exists(chombo_path+\"ready.txt\") and os.path.exists(chombo_path+filename):\n",
    "    print(\"A data file already exists in the repo. Should I delete it? y/n\")\n",
    "    dec = input()\n",
    "    if dec  == \"y\":\n",
    "            print(\"File removed \",chombo_path+filename)\n",
    "            os.remove(chombo_path+filename)\n",
    "            os.remove(chombo_path+\"ready.txt\")\n",
    "            if os.path.exists(chombo_path+\"ready.txt\") or os.path.exists(chombo_path+filename): print(\"not successful\")\n",
    "    elif dec == \"n\": print(\"file not deleted\")\n",
    "    else: print(\"This was not a viable option, run the cell again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14977fc5-d95d-46e9-80d4-ddc7f3f3faec",
   "metadata": {},
   "source": [
    "## Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c292948-8731-4b29-bcd9-23f26757707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler_filegpAMR.json\")\n",
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\" \n",
    "\n",
    "client = init_client(scheduler_file, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8116472-9dec-480f-a213-9af2090f52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8a45f-ca02-4fe7-8a34-c79a4ff67af4",
   "metadata": {},
   "source": [
    "## GP set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56acaa4-4a98-4bbe-b368-688224c3f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from gpcam.kernels import *\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Wendland C^2 compactly supported kernel (support radius = 1)\n",
    "def wendland_c2(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    out[mask] = rm**4 * (4*r[mask] + 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Derivative of Wendland C^2 w.r.t. r\n",
    "def wendland_c2_prime(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    # Ï†'(r) = d/dr [ (1-r)^4 * (4r +1) ]\n",
    "    #       = -4*(1-r)^3*(4r+1) + (1-r)^4 * 4\n",
    "    out[mask] = -4 * rm**3 * (4*r[mask] + 1) + 4 * rm**4\n",
    "    return out\n",
    "\n",
    "# Simple RBF interpolator with Wendland kernel\n",
    "class WendlandRBF:\n",
    "    def __init__(self, x, y, epsilon=0.5):\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.epsilon = epsilon\n",
    "        r = cdist(self.x, self.x) * epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        nnz = K.nnz\n",
    "\n",
    "        # Total number of entries\n",
    "        total = np.prod(K.shape)\n",
    "\n",
    "        # Sparsity = fraction of zeros\n",
    "        sparsity = nnz / total\n",
    "        self.w = spsolve(K, self.y)\n",
    "\n",
    "    def __call__(self, xnew):\n",
    "        r = cdist(np.atleast_2d(xnew), self.x) * self.epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        return np.asarray(K @ self.w)\n",
    "        \n",
    "    def gradient(self, xnew):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the RBF interpolant at one or multiple query points.\n",
    "        xnew: array of shape (M, d)\n",
    "        Returns: array of shape (M, d)\n",
    "        \"\"\"\n",
    "        xnew = np.atleast_2d(xnew)  # (M, d)\n",
    "        M, d = xnew.shape\n",
    "        N = self.x.shape[0]\n",
    "    \n",
    "        grads = np.zeros((M, d))\n",
    "    \n",
    "        for j in range(M):\n",
    "            diffs = xnew[j] - self.x        # (N, d)\n",
    "            r = np.linalg.norm(diffs, axis=1) * self.epsilon  # (N,)\n",
    "            phi_prime = wendland_c2_prime(r)\n",
    "    \n",
    "            grad = np.zeros(d)\n",
    "            for i in range(N):\n",
    "                if 0 < r[i] < 1.0:\n",
    "                    grad += self.w[i] * phi_prime[i] * self.epsilon * (diffs[i] / (r[i] + 1e-12))\n",
    "    \n",
    "            grads[j] = grad\n",
    "    \n",
    "        return np.linalg.norm(grads, axis = 1)\n",
    "\n",
    "\n",
    "def kernelPDE(x1, x2, hps, x_data = None, y_data = None):\n",
    "    st = time.time()\n",
    "    d = get_distance_matrix(x1,x2)\n",
    "    rbf = WendlandRBF(x_data, y_data, epsilon = 0.5)\n",
    "    #func1 = rbf(x1)\n",
    "    #func2 = rbf(x2)\n",
    "    func1 = griddata(global_x, global_y, x1, method = \"linear\", fill_value = 0.) #+ 0.000001\n",
    "    func2 = griddata(global_x, global_y, x2, method = \"linear\", fill_value = 0.) #+ 0.000001\n",
    "\n",
    "    #func1 = abs(rbf.gradient(x1))\n",
    "    #func2 = abs(rbf.gradient(x2))\n",
    "    \n",
    "    k = hps[0] * np.outer(func1,func2) * matern_kernel_diff1(d,hps[1])\n",
    "    #k = hps[0] * np.exp(-d/hps[1])\n",
    "    print(time.time() - st, flush = True)\n",
    "    return k\n",
    "\n",
    "def noisePDE(x, hps, x_data = None, y_data = None):\n",
    "    rbf = WendlandRBF(x_data, y_data, epsilon = 0.5)\n",
    "    func = 0.01 * (abs(rbf.gradient(x)) + 1e-6)\n",
    "    return func\n",
    "\n",
    "def meanf(x,hps):\n",
    "    return np.zeros(len(x))\n",
    "\n",
    "def acq_func(x, gp):\n",
    "    x = np.asarray(x)\n",
    "    return np.sqrt(gp.posterior_covariance(x, variance_only=True)[\"v(x)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb490ab3-eacc-4d31-8c94-dda7e9fa0e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "domain, global_x, global_y, xpatches, ypatches = read_fileII(chombo_path, filename, index, tol_ratio, delete=True, normalize = True)\n",
    "init_x_data = np.array([[domain[0,0], domain[1,0]],[domain[0,0], domain[1,1]],[domain[0,1], domain[1,0]],[domain[0,1], domain[1,1]]])\n",
    "init_y_data = np.array([0., 0., 0., 0.])\n",
    "tol = tol_ratio * np.max(abs(global_y))\n",
    "GPs = []\n",
    "candidate_pools = []\n",
    "block_domains = []\n",
    "kernels = []\n",
    "noise_funcs = []\n",
    "print(\"Initializing GPs\")\n",
    "counter = 0\n",
    "for xentry,yentry in zip(xpatches, ypatches):\n",
    "    assert len(xentry) == len(yentry)\n",
    "    #print(\"active\", end = \"....;   \")\n",
    "    #comm data to kernel func.\n",
    "    kernels.append(partial(kernelPDE, x_data = global_x, y_data = global_y))\n",
    "    noise_funcs.append(partial(noisePDE, x_data = global_x, y_data = global_y))\n",
    "    #init GPs\n",
    "    #plt.scatter(xentry[:,0], xentry[:,1], c = yentry)\n",
    "    #plt.show()\n",
    "    GPs.append(GPOptimizer(xentry, yentry, #noise_variances=(yentry * 0.001) +1e-6,\n",
    "                      noise_function=noise_funcs[-1],\n",
    "                      kernel_function=kernels[-1],\n",
    "                      prior_mean_function=meanf,\n",
    "                      init_hyperparameters = init_hyperparameters,\n",
    "                      args={\"active\": True}, calc_inv=True))\n",
    "\n",
    "\n",
    "\n",
    "    #define refinement res\n",
    "    xmin = np.min(xentry[:,0])\n",
    "    xmax = np.max(xentry[:,0])\n",
    "    ymin = np.min(xentry[:,1])\n",
    "    ymax = np.max(xentry[:,1])\n",
    "    grid = np.array(np.meshgrid(np.linspace(xmin, xmax, refinement_res * int(xmax-xmin) + 1)[:-1], np.linspace(ymin, ymax, refinement_res * int(ymax-ymin) + 1)[:-1])).T.reshape(-1, 2)\n",
    "    mask = np.all(np.isclose(grid, np.round(grid)), axis=1) #filter out all existing data grid points, THIS ONLY WORKS IF DATA POINTS ARE INTEGERS\n",
    "    grid = grid[~mask] #filter out all existing data grid points\n",
    "    candidate_pools.append([array for array in grid])\n",
    "    if not valid(candidate_pools[-1]): raise Exception(\"Invalid candidate pool\")\n",
    "    block_domains.append(np.array([[xmin, xmax],[ymin, ymax]]))\n",
    "    counter += 1\n",
    "print(\"done!\")\n",
    "\n",
    "\n",
    "all_candidates = np.vstack([np.asarray(entry) for entry in candidate_pools])\n",
    "if not valid(all_candidates): raise Exception(\"Duplicates in global candidate pool\")\n",
    "\n",
    "\n",
    "GPs = client.scatter(GPs, broadcast=False, direct=True)\n",
    "print(\"Initial training...\")\n",
    "#train_futures = []\n",
    "#for GP in GPs: train_futures.append(train(client, hyperparameter_bounds, GP,  max_iter = 100, method = \"mcmc\"))\n",
    "#client.gather(train_futures)\n",
    "#print(\"done!\")\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "iteration_counter = 0\n",
    "training_at = []\n",
    "print(\"#######################\")\n",
    "print(\"start gpAMR iteration: \")\n",
    "print(\"#######################\")\n",
    "suggestion_history = []\n",
    "while True:\n",
    "    iteration_counter += 1\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"start gpAMR iteration: \", iteration_counter)\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    res = []\n",
    "    #ASKING FOR SUGGESTIONS\n",
    "    print(\"Asking for new suggestions\")\n",
    "    for i in range(len(GPs)):\n",
    "        if not GPs[i].result().args[\"active\"]: continue\n",
    "        print(\"        Asking GP \",i, \" with \",len(GPs[i].result().x_data),\" data points, for suggestions\")\n",
    "        candidate_pool = candidate_pools[i]\n",
    "        candidates = list(chunks(candidate_pool, number_of_threads))\n",
    "        print(\"        ask for new suggestions.... Candidates  considered: \", len(candidate_pool))\n",
    "        #for chunk in candidates: res.append(ask(client, chunk, GPs[i], len(chunk), acq_func)) ###FAST BUT RAM INEFFICIENT\n",
    "        #print(np.asarray(candidate_pool))\n",
    "        res.append(ask(client, candidate_pool, GPs[i], len(candidate_pool), acq_func)) ###SLOWER BUT RAM EFFICIENT\n",
    "    new = client.gather(res)\n",
    "\n",
    "    print(\"    All GP agents reported suggestions...concatenating\")\n",
    "    SD = np.concatenate([x[\"f_a(x)\"] for x in new])\n",
    "    new = np.vstack([x[\"x\"] for x in new])\n",
    "    sorted_indices = np.argsort(SD)[::-1]\n",
    "    SD = SD[sorted_indices]\n",
    "    new = new[sorted_indices]\n",
    "    uncertainty_tol = 0.005 #np.percentile(SD, percentile) ##NEEDS IMPROVEMENTS\n",
    "    #uncertainty_tol = np.percentile(SD, percentile) ##NEEDS IMPROVEMENTS\n",
    "    plt.plot(SD)\n",
    "    print(\"uncertainty tol: \", uncertainty_tol)\n",
    "    plt.show()\n",
    "    non_zero_ind = np.where(SD > uncertainty_tol)\n",
    "    if not valid(new): raise Exception(\"Non-Unique suggestions\")\n",
    "    suggestions = new[non_zero_ind]\n",
    "    print(\"len(suggestions): \", len(suggestions))\n",
    "    print(\"    Suggestions calculated, len:\", len(suggestions))\n",
    "    print(\"    Write suggestions for Chombo iteration... \", iteration_counter)\n",
    "    #################################\n",
    "    #suggestions = np.round(suggestions)\n",
    "    #################################\n",
    "    if not valid(suggestions): raise Exception(\"Non-Unique suggestions\")\n",
    "    write_file(gpcam_path, chombo_path, suggestions) ##send to data generator (simulation)\n",
    "    print(\"    Suggestions written!\")\n",
    "    \n",
    "    suggestion_history.append(suggestions)\n",
    "    \n",
    "\n",
    "    #PLOTTING\n",
    "    if plotting:\n",
    "        print(\"Generating plots...\")\n",
    "        mean_futures = []\n",
    "        cov_futures = []\n",
    "        for i in range(len(GPs)):\n",
    "            if not GPs[i].result().args[\"active\"]: continue \n",
    "            x_plot = GPOptimizer.make_2d_x_pred(block_domains[i][0],block_domains[i][1],resx=plot_resx,resy=plot_resy)\n",
    "            mean_futures.append(posterior_mean(client, x_plot,GPs[i]))\n",
    "            cov_futures.append(posterior_covariance(client, x_plot,GPs[i]))\n",
    "        means_tmp = client.gather(mean_futures)\n",
    "        means = np.concatenate([mean[\"m(x)\"] for mean in means_tmp])\n",
    "        cov_tmp = client.gather(cov_futures)\n",
    "        stds = np.concatenate([np.sqrt(cov[\"v(x)\"]) for cov in cov_tmp])\n",
    "        x_pred = np.vstack([mean[\"x_pred\"] for mean in means_tmp])\n",
    "        \n",
    "        print(\"data and suggestions:\")\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(global_x[:,0], global_x[:,1], c=global_y, alpha=.2)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.1, c='black', alpha=1.)\n",
    "        plt.colorbar(a)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"mean and suggestions:\")\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plot2d(x_pred[:,0], x_pred[:,1], means, suggestions=suggestions, title= \"mean and suggestions\") #,filename=gpcam_path+\"mean\" + str(iteration_counter).zfill(4))\n",
    "        \n",
    "        print(\"std and suggestions:\")\n",
    "        plot2d(x_pred[:,0], x_pred[:,1], stds, suggestions=suggestions, title= \"std and suggestions\") #, filename=gpcam_path+\"std\" + str(iteration_counter).zfill(4))\n",
    "\n",
    "        ##write image to disc\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(x_pred[:,0],x_pred[:,1], c = means, alpha=0.5)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.2, c='black', alpha=0.5)\n",
    "        plt.xlim(domain[0,0], domain[0,1])\n",
    "        plt.ylim(domain[1,0], domain[1,1])\n",
    "        plt.colorbar(a)\n",
    "        plt.savefig(gpcam_path+\"mean_sugg\" + str(iteration_counter).zfill(4))\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(x_pred[:,0],x_pred[:,1], c = stds, alpha=0.5)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.2, c='black', alpha=0.5)\n",
    "        plt.xlim(domain[0,0], domain[0,1])\n",
    "        plt.ylim(domain[1,0], domain[1,1])\n",
    "        plt.colorbar(a)\n",
    "        plt.savefig(gpcam_path+\"std_sugg\" + str(iteration_counter).zfill(4))\n",
    "        plt.show()\n",
    "        print(\"Done!\")\n",
    "\n",
    "    #break\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"Reading Chombo file. Iteration: \", iteration_counter)\n",
    "    print(\"reading: \", filename)\n",
    "    domain, global_x, global_y, xpatches, ypatches = read_fileII(chombo_path, filename, index, tol_ratio, delete=True, normalize = True) ##immediately look for new data and read when available\n",
    "    print(\"global dataset size: \", len(global_x))\n",
    "    print(\"done!\")\n",
    "\n",
    "    \n",
    "    #UPDATE GPs\n",
    "    print(\"Updating GP agents...\")\n",
    "    update_futures = [] \n",
    "    for i in range(len(GPs)):\n",
    "        #comm data to kernel func.\n",
    "            kernels[i] = partial(kernelPDE, x_data = global_x, y_data = global_y)\n",
    "            noise_funcs[i] = partial(noisePDE, x_data = global_x, y_data = global_y)\n",
    "            set_new_kernel(client, kernels[i], GPs[i])\n",
    "            set_new_noise_func(client, noise_funcs[i], GPs[i])\n",
    "            set_args(client, GPs[i], {\"active\": True})\n",
    "            update_futures.append(tell(client, xpatches[i], ypatches[i], None, GPs[i])) #, (yentry * 0.001) +1e-6, GPs[i]))\n",
    "\n",
    "    client.gather(update_futures)\n",
    "    print(\"Updating GP agents done!\")\n",
    "\n",
    "    #TRAINING\n",
    "    if iteration_counter in training_at:\n",
    "        print(\"Training...\")\n",
    "        train_futures = []\n",
    "        for GP in GPs: \n",
    "            if not GPs[i].result().args[\"active\"]: continue\n",
    "            train_futures.append(train(client, hyperparameter_bounds, GPs[i],  max_iter = 100, method = \"mcmc\"))\n",
    "        client.gather(train_futures)\n",
    "        print(\"Training done!\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f50a6a-7b3a-45c2-9ed6-b2b58908e050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5ebc5c-6e39-45d6-9725-f41f3f67a578",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a944df2-aba5-4b4d-91d6-e9fbc43281c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad529b8b-c1c4-41c2-8e10-ecf97a7d62f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpAMRenv",
   "language": "python",
   "name": "gpamrenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
