{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249d9f2-a705-4f1e-9bac-4a86e6468679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "# Set field size limit to the maximum possible\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import ast\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "from read_data import *\n",
    "from gpcam import GPOptimizer\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from functools import partial\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "import os\n",
    "import time\n",
    "from gpAMRutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd4591-be09-407d-b1e8-907091c6c08d",
   "metadata": {},
   "source": [
    "## Tuning options to consider\n",
    "- Filtering data by block or as a function of the global data?\n",
    "- What fraction of candidates should be used, > mean SD? top 10%, top n candidates?\n",
    "- How will data noise be estimated and set?\n",
    "- RAM: submitting ask several times means that $\\kappa \\in \\mathbb{R}^{12000 \\times 12000}$ is possibly stored many times at once. At the same time the interpolator in the kernel stores temporarily 2(3) matrices of shape (len(x_data)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85100655-9117-46b9-b44d-ca62b9d2356b",
   "metadata": {},
   "source": [
    "## SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9b443-3b02-434a-9118-62dfe57b893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings:\n",
    "number_of_workers = 256\n",
    "number_of_threads = 32\n",
    "\n",
    "tol_ratio = 1e-2 #remove data < tol_ratio * max(data)\n",
    "refinement_res = 2 #refinement level\n",
    "plotting = True #plot posterior mean and var\n",
    "plot_resx = 100\n",
    "plot_resy =  50\n",
    "\n",
    "#filename = \"plot.nx256.2d.AMRLevel.hdf5\"\n",
    "filename = \"plot.nx2048.2d.AMRLevel.hdf5\"\n",
    "index = \"vorticity\"\n",
    "percentile = 99 ##of all candidates considered, return the top percentile\n",
    "\n",
    "chombo_path = \"./ChomboOut/\"\n",
    "gpcam_path = \"./gpCAMOut/\"\n",
    "\n",
    "\n",
    "init_hyperparameters = np.array([1., 20., 0.5])\n",
    "hyperparameter_bounds = np.array([[0.1, 10.],\n",
    "                                  [0.1, 10.],\n",
    "                                  [0.1, 0.5],])\n",
    "\n",
    "if os.path.exists(chombo_path+\"ready.txt\") and os.path.exists(chombo_path+filename):\n",
    "    print(\"A data file already exists in the repo. Should I delete it? y/n\")\n",
    "    dec = input()\n",
    "    if dec  == \"y\":\n",
    "            print(\"File removed ...\")\n",
    "            os.remove(chombo_path+filename)\n",
    "            os.remove(chombo_path+\"ready.txt\")\n",
    "    elif dec == \"n\": print(\"file not deleted\")\n",
    "    else: print(\"This was not a viable option, run the cell again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14977fc5-d95d-46e9-80d4-ddc7f3f3faec",
   "metadata": {},
   "source": [
    "## Dask Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c292948-8731-4b29-bcd9-23f26757707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler_filegpAMR.json\")\n",
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\" \n",
    "\n",
    "client = init_client(scheduler_file, number_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8116472-9dec-480f-a213-9af2090f52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8a45f-ca02-4fe7-8a34-c79a4ff67af4",
   "metadata": {},
   "source": [
    "## GP set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56acaa4-4a98-4bbe-b368-688224c3f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from gpcam.kernels import *\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Wendland C^2 compactly supported kernel (support radius = 1)\n",
    "def wendland_c2(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    out[mask] = rm**4 * (4*r[mask] + 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Derivative of Wendland C^2 w.r.t. r\n",
    "def wendland_c2_prime(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    # Ï†'(r) = d/dr [ (1-r)^4 * (4r +1) ]\n",
    "    #       = -4*(1-r)^3*(4r+1) + (1-r)^4 * 4\n",
    "    out[mask] = -4 * rm**3 * (4*r[mask] + 1) + 4 * rm**4\n",
    "    return out\n",
    "\n",
    "# Simple RBF interpolator with Wendland kernel\n",
    "class WendlandRBF:\n",
    "    def __init__(self, x, y, epsilon=0.5):\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.epsilon = epsilon\n",
    "        r = cdist(self.x, self.x) * epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        nnz = K.nnz\n",
    "\n",
    "        # Total number of entries\n",
    "        total = np.prod(K.shape)\n",
    "\n",
    "        # Sparsity = fraction of zeros\n",
    "        sparsity = nnz / total\n",
    "        self.w = spsolve(K, self.y)\n",
    "\n",
    "    def __call__(self, xnew):\n",
    "        r = cdist(np.atleast_2d(xnew), self.x) * self.epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        return np.asarray(K @ self.w)\n",
    "        \n",
    "    def gradient(self, xnew):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the RBF interpolant at one or multiple query points.\n",
    "        xnew: array of shape (M, d)\n",
    "        Returns: array of shape (M, d)\n",
    "        \"\"\"\n",
    "        xnew = np.atleast_2d(xnew)  # (M, d)\n",
    "        M, d = xnew.shape\n",
    "        N = self.x.shape[0]\n",
    "    \n",
    "        grads = np.zeros((M, d))\n",
    "    \n",
    "        for j in range(M):\n",
    "            diffs = xnew[j] - self.x        # (N, d)\n",
    "            r = np.linalg.norm(diffs, axis=1) * self.epsilon  # (N,)\n",
    "            phi_prime = wendland_c2_prime(r)\n",
    "    \n",
    "            grad = np.zeros(d)\n",
    "            for i in range(N):\n",
    "                if 0 < r[i] < 1.0:\n",
    "                    grad += self.w[i] * phi_prime[i] * self.epsilon * (diffs[i] / (r[i] + 1e-12))\n",
    "    \n",
    "            grads[j] = grad\n",
    "    \n",
    "        return np.linalg.norm(grads, axis = 1)\n",
    "\n",
    "\n",
    "def kernelPDE(x1, x2, hps, x_data = None, y_data = None):\n",
    "    #st = time.time()\n",
    "    rbf = WendlandRBF(x_data[::2], y_data[::2], epsilon = hps[2]/2.)\n",
    "    #print(time.time() - st, flush = True)\n",
    "    func1 = abs(rbf(x1))\n",
    "    func2 = abs(rbf(x2))\n",
    "\n",
    "    #func1 = abs(rbf.gradient(x1))\n",
    "    #func2 = abs(rbf.gradient(x2))\n",
    "    del rbf\n",
    "    d = get_distance_matrix(x1,x2)\n",
    "    k = hps[0] * np.outer(func1,func2) * matern_kernel_diff1(d,hps[1])\n",
    "    return k \n",
    "\n",
    "def meanf(x,hps):\n",
    "    return np.zeros(len(x))\n",
    "\n",
    "def acq_func(x, gp):\n",
    "    x = np.asarray(x)\n",
    "    return np.sqrt(gp.posterior_covariance(x, variance_only=True)[\"v(x)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb490ab3-eacc-4d31-8c94-dda7e9fa0e7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets, domain, global_x, global_y = read_file(chombo_path, filename, index, tol_ratio)\n",
    "init_x_data = np.array([[domain[0,0], domain[1,0]],[domain[0,0], domain[1,1]],[domain[0,1], domain[1,0]],[domain[0,1], domain[1,1]]])\n",
    "init_y_data = np.array([0., 0., 0., 0.])\n",
    "tol = tol_ratio * np.max(abs(global_y))\n",
    "\n",
    "GPs = {}\n",
    "candidate_pools = {}\n",
    "block_domains = {}\n",
    "kernels = {}\n",
    "print(\"Initializing GPs\")\n",
    "for ID in datasets:\n",
    "    assert len(datasets[ID][0]) == len(datasets[ID][1])\n",
    "    #print(ID, end = ' ')\n",
    "    if len(datasets[ID][0]) >= 4:\n",
    "        #print(\"active\", end = \"....;   \")\n",
    "        #comm data to kernel func.\n",
    "        kernels[ID] = partial(kernelPDE, x_data = global_x, y_data = global_y)\n",
    "        #init GPs\n",
    "        GPs[ID] = GPOptimizer(datasets[ID][0], datasets[ID][1], noise_variances=np.ones(datasets[ID][1].shape) * 0.1,\n",
    "                          kernel_function=kernels[ID],\n",
    "                          prior_mean_function=meanf,\n",
    "                          init_hyperparameters = init_hyperparameters,\n",
    "                          args={\"active\": True}, calc_inv=True)\n",
    "\n",
    "    else:\n",
    "        #comm data to kernel func.\n",
    "        #print(\"inactive\", end = \"....;   \")\n",
    "        kernels[ID] = partial(kernelPDE, x_data = global_x, y_data = global_y)\n",
    "        #init GPs\n",
    "        GPs[ID] = GPOptimizer(init_x_data, init_y_data, noise_variances=np.ones(init_y_data.shape) * 0.1,\n",
    "                          kernel_function=kernels[ID],\n",
    "                          prior_mean_function=meanf,\n",
    "                          init_hyperparameters = init_hyperparameters,\n",
    "                          args={\"active\": False}, calc_inv=True)\n",
    "\n",
    "    #define refinement res\n",
    "    xmin = datasets[ID][2][0,0]\n",
    "    xmax = datasets[ID][2][0,1]\n",
    "    ymin = datasets[ID][2][1,0]\n",
    "    ymax = datasets[ID][2][1,1]\n",
    "    grid = np.array(np.meshgrid(np.linspace(xmin, xmax, refinement_res * int(xmax-xmin) + 1)[:-1], np.linspace(ymin, ymax, refinement_res * int(ymax-ymin) + 1)[:-1])).T.reshape(-1, 2)\n",
    "    mask = np.all(np.isclose(grid, np.round(grid)), axis=1) #filter out all existing data grid points, THIS ONLY WORKS IF DATA POINTS ARE INTEGERS\n",
    "    grid = grid[~mask] #filter out all existing data grid points\n",
    "    candidate_pools[ID] = [array for array in grid]\n",
    "    if not valid(candidate_pools[ID]): raise Exception(\"Invalid candidate pool in ID \", ID)\n",
    "    block_domains[ID] = datasets[ID][2]\n",
    "print(\"done!\")\n",
    "\n",
    "\n",
    "all_candidates = np.vstack([np.asarray(candidate_pools[ID]) for ID in datasets])\n",
    "if not valid(all_candidates): raise Exception(\"Duplicates in global candidate pool\")\n",
    "\n",
    "\n",
    "GPs = client.scatter(GPs, broadcast=False, direct=True)\n",
    "print(\"Initial training...\")\n",
    "train_futures = []\n",
    "for ID in datasets: \n",
    "    train_futures.append(train(client, hyperparameter_bounds, GPs[ID],  max_iter = 1000, method = \"mcmc\"))\n",
    "client.gather(train_futures)\n",
    "print(\"done!\")\n",
    "\n",
    "\n",
    "\n",
    "###################################################\n",
    "###################################################\n",
    "iteration_counter = 0\n",
    "training_at = [1,2,3,4,5, 10]\n",
    "print(\"#######################\")\n",
    "print(\"start gpAMR iteration: \")\n",
    "print(\"#######################\")\n",
    "suggestion_history = []\n",
    "while True:\n",
    "    iteration_counter += 1\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"start gpAMR iteration: \", iteration_counter)\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    \n",
    "    res = []\n",
    "    #ASKING FOR SUGGESTIONS\n",
    "    print(\"Asking for new suggestions\")\n",
    "    for ID in datasets:\n",
    "        if not GPs[ID].result().args[\"active\"]: continue\n",
    "        print(\"        Asking GP \",ID, \" with \",len(GPs[ID].result().x_data),\" data points, for suggestions\")\n",
    "        candidate_pool = candidate_pools[ID]\n",
    "        #candidates = list(chunks(candidate_pool, number_of_threads))\n",
    "        print(\"        ask for new suggestions.... Candidates  considered: \", len(candidate_pool))\n",
    "        #for chunk in candidates: res.append(ask(client, chunk, GPs[ID], len(chunk), acq_func)) ###FAST BUT RAM INEFFICIENT\n",
    "        #print(np.asarray(candidate_pool))\n",
    "        res.append(ask(client, candidate_pool, GPs[ID], len(candidate_pool), acq_func)) ###SLOWER BUT RAM EFFICIENT\n",
    "    new = client.gather(res)\n",
    "\n",
    "    print(\"    All GP agents reported suggestions...concatenating\")\n",
    "    SD = np.concatenate([x[\"f_a(x)\"] for x in new])\n",
    "    new = np.vstack([x[\"x\"] for x in new])\n",
    "    sorted_indices = np.argsort(SD)[::-1]\n",
    "    SD = SD[sorted_indices]\n",
    "    new = new[sorted_indices]\n",
    "    uncertainty_tol = 0.01 #np.percentile(SD, percentile) ##NEEDS IMPROVEMENTS\n",
    "    plt.plot(SD)\n",
    "    print(\"uncertainty tol: \", uncertainty_tol)\n",
    "    plt.show()\n",
    "    non_zero_ind = np.where(SD > uncertainty_tol)\n",
    "    if not valid(new): raise Exception(\"Non-Unique suggestions\")\n",
    "    suggestions = new[non_zero_ind]\n",
    "    print(\"len(suggestions): \", len(suggestions))\n",
    "    print(\"    Suggestions calculated, len:\", len(suggestions))\n",
    "    print(\"    Write suggestions for Chombo iteration... \", iteration_counter)\n",
    "    #################################\n",
    "    #suggestions = np.round(suggestions)\n",
    "    #################################\n",
    "    if not valid(suggestions): raise Exception(\"Non-Unique suggestions\")\n",
    "    write_file(gpcam_path, chombo_path, suggestions) ##send to data generator (simulation)\n",
    "    print(\"    Suggestions written!\")\n",
    "    \n",
    "\n",
    "    suggestion_history.append(suggestions)\n",
    "    \n",
    "\n",
    "    #PLOTTING\n",
    "    if plotting:\n",
    "        print(\"Generating plots...\")\n",
    "        mean_futures = []\n",
    "        cov_futures = []\n",
    "        for ID in datasets:\n",
    "            if not GPs[ID].result().args[\"active\"]: continue \n",
    "            x_plot = GPOptimizer.make_2d_x_pred(block_domains[ID][0],block_domains[ID][1],resx=plot_resx,resy=plot_resy)\n",
    "            mean_futures.append(posterior_mean(client, x_plot,GPs[ID]))\n",
    "            cov_futures.append(posterior_covariance(client, x_plot,GPs[ID]))\n",
    "        means_tmp = client.gather(mean_futures)\n",
    "        means = np.concatenate([mean[\"m(x)\"] for mean in means_tmp])\n",
    "        cov_tmp = client.gather(cov_futures)\n",
    "        stds = np.concatenate([np.sqrt(cov[\"v(x)\"]) for cov in cov_tmp])\n",
    "        x_pred = np.vstack([mean[\"x_pred\"] for mean in means_tmp])\n",
    "        \n",
    "        print(\"data and suggestions:\")\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(global_x[:,0], global_x[:,1], c=global_y, alpha=.2)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.1, c='black', alpha=1.)\n",
    "        plt.colorbar(a)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(20,5))\n",
    "        plot2d(x_pred[:,0], x_pred[:,1], means, suggestions=suggestions, title= \"mean and suggestions\",filename=gpcam_path+\"mean\" + str(iteration_counter).zfill(4))\n",
    "        \n",
    "        print(\"std and suggestions:\")\n",
    "        plot2d(x_pred[:,0], x_pred[:,1], stds, suggestions=suggestions, title= \"std and suggestions\", filename=gpcam_path+\"std\" + str(iteration_counter).zfill(4))\n",
    "\n",
    "        ##write image to disc\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(x_pred[:,0],x_pred[:,1],c = means, alpha=0.5, vmin=-60, vmax=60)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.1, c='black', alpha=0.1)\n",
    "        plt.xlim(domain[0,0], domain[0,1])\n",
    "        plt.ylim(domain[1,0], domain[1,1])\n",
    "        plt.colorbar(a)\n",
    "        plt.savefig(gpcam_path+\"mean_sugg\" + str(iteration_counter).zfill(4))\n",
    "        plt.show()\n",
    "        ##write image to disc\n",
    "        plt.figure(figsize=(20,5))\n",
    "        a = plt.scatter(x_pred[:,0],x_pred[:,1],c = stds, alpha=0.5, vmin=-60, vmax=60)\n",
    "        plt.scatter(suggestions[:,0], suggestions[:,1], s=0.1, c='black', alpha=0.1)\n",
    "        plt.xlim(domain[0,0], domain[0,1])\n",
    "        plt.ylim(domain[1,0], domain[1,1])\n",
    "        plt.colorbar(a)\n",
    "        plt.savefig(gpcam_path+\"std_sugg\" + str(iteration_counter).zfill(4))\n",
    "        plt.show()\n",
    "        print(\"Done!\")\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"Reading Chombo file. Iteration: \", iteration_counter)\n",
    "    datasets, domain, global_x, global_y = read_file(chombo_path, filename, index, tol_ratio) ##immediately look for new data and read when available\n",
    "    print(\"global dataset size: \", len(global_x))\n",
    "    print(\"data:\")\n",
    "    print(\"filter tol: \", tol_ratio * np.max(global_y))\n",
    "    plt.figure(figsize=(20,5))\n",
    "    a = plt.scatter(global_x[:,0], global_x[:,1], c=global_y, alpha=.2)\n",
    "    plt.colorbar(a)\n",
    "    plt.show()\n",
    "    print(\"done!\")\n",
    "\n",
    "    \n",
    "    #UPDATE GPs\n",
    "    print(\"Updating GP agents...\")\n",
    "    update_futures = [] \n",
    "    for ID in datasets:\n",
    "        #comm data to kernel func.\n",
    "        if len(datasets[ID][0])>4:\n",
    "            kernels[ID] = partial(kernelPDE, x_data = global_x, y_data = global_y)\n",
    "            set_new_kernel(client, kernels[ID], GPs[ID])\n",
    "            set_args(client, GPs[ID], {\"active\": True})\n",
    "            update_futures.append(tell(client, datasets[ID][0], datasets[ID][1], np.ones(datasets[ID][1].shape) * 0.1, GPs[ID]))\n",
    "            \n",
    "        else:\n",
    "            kernels[ID] = partial(kernelPDE, x_data = global_x, y_data = global_y)\n",
    "            set_new_kernel(client, kernels[ID], GPs[ID])\n",
    "            set_args(client, GPs[ID], {\"active\": False})\n",
    "            update_futures.append(tell(client, init_x_data, init_y_data, np.ones(init_y_data.shape) * 0.1, GPs[ID]))\n",
    "    client.gather(update_futures)\n",
    "    print(\"Updating GP agents done!\")\n",
    "\n",
    "    #TRAINING\n",
    "    if iteration_counter in training_at:\n",
    "        print(\"Training...\")\n",
    "        train_futures = []\n",
    "        for ID in datasets: \n",
    "            if not GPs[ID].result().args[\"active\"]: continue\n",
    "            train_futures.append(train(client, hyperparameter_bounds, GPs[ID],  max_iter = 100, method = \"mcmc\"))\n",
    "        client.gather(train_futures)\n",
    "        print(\"Training done!\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b231f-088b-4a21-a69b-919093b2e652",
   "metadata": {},
   "source": [
    "## Building intuition of our kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a3bdf-2624-4f5e-bcab-d4328087ac60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "x_pred1D = np.linspace(0,1,1000).reshape(-1,1)\n",
    "\n",
    "x = np.linspace(0,600,1000)\n",
    "def f1(x):\n",
    "    return np.sin(5. * x) + np.cos(10. * x) + (2.* (x-0.4)**2) * np.cos(100. * x)\n",
    "\n",
    "x_data = np.linspace(0,1,20).reshape(20,1)\n",
    "y_data = f1(x_data[:,0]) + (np.random.rand(len(x_data))-0.5) * 0.1\n",
    "\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.xticks([0.,0.5,1.0])\n",
    "plt.yticks([-2,-1,0.,1])\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.plot(x_pred1D,f1(x_pred1D), color = 'orange', linewidth = 4)\n",
    "plt.scatter(x_data[:,0],y_data, color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16caff78-bf1c-447d-afe9-ade49cdaa4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import griddata\n",
    "from gpcam.kernels import *\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Wendland C^2 compactly supported kernel (support radius = 1)\n",
    "def wendland_c2(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    out[mask] = rm**4 * (4*r[mask] + 1)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Derivative of Wendland C^2 w.r.t. r\n",
    "def wendland_c2_prime(r):\n",
    "    out = np.zeros_like(r)\n",
    "    mask = r < 1.0\n",
    "    rm = 1 - r[mask]\n",
    "    # Ï†'(r) = d/dr [ (1-r)^4 * (4r +1) ]\n",
    "    #       = -4*(1-r)^3*(4r+1) + (1-r)^4 * 4\n",
    "    out[mask] = -4 * rm**3 * (4*r[mask] + 1) + 4 * rm**4\n",
    "    return out\n",
    "\n",
    "# Simple RBF interpolator with Wendland kernel\n",
    "class WendlandRBF:\n",
    "    def __init__(self, x, y, epsilon=0.5):\n",
    "        self.x = np.asarray(x)\n",
    "        self.y = np.asarray(y)\n",
    "        self.epsilon = epsilon\n",
    "        r = cdist(self.x, self.x) * epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        nnz = K.nnz\n",
    "\n",
    "        # Total number of entries\n",
    "        total = np.prod(K.shape)\n",
    "\n",
    "        # Sparsity = fraction of zeros\n",
    "        sparsity = nnz / total\n",
    "        self.w = spsolve(K, self.y)\n",
    "\n",
    "    def __call__(self, xnew):\n",
    "        r = cdist(np.atleast_2d(xnew), self.x) * self.epsilon\n",
    "        K = wendland_c2(r)\n",
    "        K = csr_matrix(K)\n",
    "        return np.asarray(K @ self.w)\n",
    "        \n",
    "    def gradient(self, xnew):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the RBF interpolant at one or multiple query points.\n",
    "        xnew: array of shape (M, d)\n",
    "        Returns: array of shape (M, d)\n",
    "        \"\"\"\n",
    "        xnew = np.atleast_2d(xnew)  # (M, d)\n",
    "        M, d = xnew.shape\n",
    "        N = self.x.shape[0]\n",
    "    \n",
    "        grads = np.zeros((M, d))\n",
    "    \n",
    "        for j in range(M):\n",
    "            diffs = xnew[j] - self.x        # (N, d)\n",
    "            r = np.linalg.norm(diffs, axis=1) * self.epsilon  # (N,)\n",
    "            phi_prime = wendland_c2_prime(r)\n",
    "    \n",
    "            grad = np.zeros(d)\n",
    "            for i in range(N):\n",
    "                if 0 < r[i] < 1.0:\n",
    "                    grad += self.w[i] * phi_prime[i] * self.epsilon * (diffs[i] / (r[i] + 1e-12))\n",
    "    \n",
    "            grads[j] = grad\n",
    "    \n",
    "        return np.linalg.norm(grads, axis = 1)\n",
    "\n",
    "\n",
    "def kernelPDE(x1, x2, hps, x_data = None, y_data = None):\n",
    "    #st = time.time()\n",
    "    d = get_distance_matrix(x1,x2)\n",
    "    rbf = WendlandRBF(x_data, y_data, epsilon = hps[2])\n",
    "    #print(time.time() - st, flush = True)\n",
    "    #func1 = abs(rbf(x1))\n",
    "    #func2 = abs(rbf(x2))\n",
    "\n",
    "    func1 = rbf.gradient(x1)\n",
    "    func2 = rbf.gradient(x2)\n",
    "    \n",
    "    k = hps[0] * np.outer(func1,func2) * matern_kernel_diff1(d,hps[1])\n",
    "    return k \n",
    "\n",
    "def meanf(x,hps):\n",
    "    return np.zeros(len(x))\n",
    "\n",
    "\n",
    "\n",
    "kernel = partial(kernelPDE, x_data = x_data, y_data = y_data)\n",
    "init_hyperparameters = np.array([1., 20., 0.05])\n",
    "hyperparameter_bounds = np.array([[0.1, 10.],\n",
    "                                  [0.1, 10.],\n",
    "                                  [0.1, 0.5],])\n",
    "\n",
    "my_gp1 = GPOptimizer(x_data, y_data, noise_variances=np.ones(y_data.shape) * 0.01,\n",
    "                          kernel_function=kernel,\n",
    "                          prior_mean_function=meanf,\n",
    "                          init_hyperparameters = init_hyperparameters,\n",
    "                          args={\"active\": True}, calc_inv=True)\n",
    "my_gp1.train(hyperparameter_bounds=hyperparameter_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff9da6-e32c-468f-9f25-893ed669a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make a prediction\n",
    "x_pred = x_pred1D.flatten()\n",
    "\n",
    "mean1 = my_gp1.posterior_mean(x_pred.reshape(-1,1))[\"m(x)\"]\n",
    "var1 =  my_gp1.posterior_covariance(x_pred.reshape(-1,1), variance_only=False, add_noise=True)[\"v(x)\"]\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.plot(x_pred,mean1, label = \"posterior mean\", linewidth = 4)\n",
    "plt.plot(x_pred1D,f1(x_pred1D), label = \"latent function\", linewidth = 4)\n",
    "plt.fill_between(x_pred, mean1 - 3. * np.sqrt(var1), mean1 + 3. * np.sqrt(var1), alpha = 0.5, color = \"grey\", label = \"var\")\n",
    "plt.scatter(x_data,y_data, color = 'black')\n",
    "\n",
    "\n",
    "##looking at some validation metrics\n",
    "print(my_gp1.rmse(x_pred1D,f1(x_pred1D).reshape(1000)))\n",
    "print(my_gp1.crps(x_pred1D,f1(x_pred1D).reshape(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8882e7-65f3-4636-8cef-71b97dd661ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = WendlandRBF(x_data, y_data, epsilon = 0.4487)\n",
    "func1 = abs(rbf.gradient(x_pred1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9c01d-647a-45b1-896c-305f05dc7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred1D,func1, label = \"interpolant\", linewidth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b0fe6-345b-4975-ae7f-11bb0b26eeef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13accca2-e22d-444b-8d60-80661a1e4597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpAMRenv",
   "language": "python",
   "name": "gpamrenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
